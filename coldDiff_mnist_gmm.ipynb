{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab only\n",
    "# %pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install transformers accelerate datasets\n",
    "# %pip install tqdm\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install opencv-python\n",
    "# %pip install tensorboardX\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01994b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from torchvision.transforms import GaussianBlur\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, zoom, mean as ndimage_mean\n",
    "from skimage.transform import resize\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f27705",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c984c85",
   "metadata": {},
   "source": [
    "## Functions for degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e51de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for pixellation\n",
    "\n",
    "def upsample_nearest(image, new_width, new_height):\n",
    "\n",
    "    old_height, old_width = image.shape[-2:]\n",
    "    scale_x = new_width / old_width\n",
    "    scale_y = new_height / old_height\n",
    "\n",
    "    # Create the coordinate grid for the new image\n",
    "    x_indices = (np.arange(new_width) / scale_x).astype(int)\n",
    "    y_indices = (np.arange(new_height) / scale_y).astype(int)\n",
    "\n",
    "    # Use advanced indexing to map the input pixels to the new grid\n",
    "    upsampled_image = image[:,y_indices[:, None], x_indices]\n",
    "    return upsampled_image\n",
    "\n",
    "def downsample_to_fixed_size(image, target_size):\n",
    "\n",
    "    num_channels, input_height, input_width = image.shape\n",
    "    target_rows, target_cols = target_size\n",
    "\n",
    "    # Calculate the scaling factors\n",
    "    row_scale = input_height / target_rows\n",
    "    col_scale = input_width / target_cols\n",
    "\n",
    "    # Create an empty array for the downsampled image\n",
    "    downsampled_image = np.zeros((num_channels,target_rows, target_cols), dtype=np.float32)\n",
    "\n",
    "    for row in range(target_rows):\n",
    "        for col in range(target_cols):\n",
    "            # Determine the boundaries of the region in the original image\n",
    "            row_start = int(row * row_scale)\n",
    "            row_end = int((row + 1) * row_scale)\n",
    "            col_start = int(col * col_scale)\n",
    "            col_end = int((col + 1) * col_scale)\n",
    "\n",
    "            # Extract the region and compute the average for each channel\n",
    "            region = image[:,row_start:row_end, col_start:col_end]\n",
    "            downsampled_image[:,row, col] = region.mean(axis=(1, 2))\n",
    "\n",
    "    return downsampled_image.astype(image.dtype)\n",
    "\n",
    "# Taken from https://github.com/hendrycks/robustness/blob/master/ImageNet-C/imagenet_c/imagenet_c/corruptions.py\n",
    "def snow(x0, severity=1):\n",
    "    c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n",
    "         (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n",
    "         (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n",
    "         (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n",
    "         (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n",
    "\n",
    "    x0 = np.array(x0, dtype=np.float32) / 255.\n",
    "    snow_layer = np.random.normal(size=x0.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n",
    "\n",
    "    #snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n",
    "    snow_layer[snow_layer < c[3]] = 0\n",
    "\n",
    "    snow_layer = np.clip((snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "    #snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n",
    "\n",
    "    snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n",
    "                              cv2.IMREAD_UNCHANGED) / 255.\n",
    "    snow_layer = snow_layer[..., np.newaxis]\n",
    "\n",
    "    x0 = c[6] * x0 + (1 - c[6]) * np.maximum(x0, cv2.cvtColor(x0, cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n",
    "    return np.clip(x0 + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060007cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class coldDiff:\n",
    "    def __init__(self, steps=300, size=28, loss_type='L1',degradation_type='blur'):\n",
    "    \n",
    "        self.steps = steps\n",
    "        self.size = size\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.beta_schedule = self.get_beta_schedule(self.steps)\n",
    "        self.alpha = 1.0 - self.beta_schedule\n",
    "        alphas = torch.cat([torch.tensor([1.]), self.alpha])  # Add a leading 1.0 to the alphas tensor\n",
    "        self.alpha_hat = torch.cumprod(alphas, dim=0).to(device)\n",
    "\n",
    "        # Degradation type can be 'blur', 'pixellate', 'inpainting' or 'snow'\n",
    "        if degradation_type == 'blur':\n",
    "            self.degradation = self.blur\n",
    "        elif degradation_type == 'pixellate':\n",
    "            self.degradation = self.pixellate\n",
    "        elif degradation_type == 'inpainting':\n",
    "            self.degradation = self.inpainting\n",
    "        elif degradation_type == 'snow':\n",
    "            self.degradation = self.snow\n",
    "        elif degradation_type == 'gaussian':\n",
    "            self.degradation = self.gaussian_noise\n",
    "        else:\n",
    "            raise ValueError(\"Invalid degradation type. Choose from 'blur', 'pixellate', 'inpainting' or 'snow'.\")\n",
    "\n",
    "    # SAMPLING FUNCTIONS\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.steps, size=(n,)) #Should this be self.steps+1?\n",
    "    \n",
    "    def sample(self, model, batch_size,initial_image='real_degraded', data_loader=None, gmm=None):\n",
    "        output_shape = (batch_size, 1, 28, 28)\n",
    "        t = self.steps\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            #x_prev = x_t\n",
    "            # black image\n",
    "            if initial_image == 'black':\n",
    "                x_prev = torch.zeros(output_shape).to(device)\n",
    "\n",
    "            if initial_image == 'random':\n",
    "                # random and degraded\n",
    "                dummy_x0 = torch.randn(output_shape).to(device)  # Or: real_batch_from_train_loader\n",
    "                timestep_tensor = torch.tensor([self.steps] * batch_size).to(device)\n",
    "                x_prev = self.degradation(dummy_x0, timestep_tensor)\\\n",
    "            \n",
    "            if initial_image == 'real_degraded':\n",
    "                if data_loader is None:\n",
    "                    raise ValueError(\"data_loader is not provided. Please provide a DataLoader for sampling.\")\n",
    "                real_batch, _ = next(iter(data_loader))  # get real MNIST digits\n",
    "                real_batch = real_batch.to(device)\n",
    "                real_batch = real_batch[:batch_size]  # in case batch size mismatch\n",
    "                timestep_tensor = torch.tensor([self.steps] * real_batch.shape[0]).to(device)\n",
    "                x_prev = self.degradation(real_batch, timestep_tensor)\n",
    "\n",
    "            if initial_image == 'gmm':\n",
    "                # Gaussian Mixture Model\n",
    "                if gmm is None:\n",
    "                    raise ValueError(\"GMM is not provided. Please provide a GMM model for sampling.\")\n",
    "                # Sample from the GMM\n",
    "                gmm_samples, _ = gmm.sample(batch_size)  # Sample from the GMM\n",
    "                gmm_samples = gmm_samples.astype(np.float32)\n",
    "                gmm_samples = torch.from_numpy(gmm_samples)  # Convert to tensor\n",
    "                gmm_samples = gmm_samples.view(batch_size, 1, self.size, self.size)  # Reshape to the desired output shape\n",
    "                gmm_samples = gmm_samples.to(device)  # Move to the appropriate device\n",
    "                # Set the initial image to the GMM samples\n",
    "                x_prev = gmm_samples\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            for s in range(t, 0, -1):\n",
    "                s_ = (torch.ones(batch_size)*s).long().to(device)\n",
    "                pred_x0 = model(x_prev, s_)\n",
    "                x_prev = x_prev - self.degradation(pred_x0, s_) + self.degradation(pred_x0, s_-1)\n",
    "        return x_prev\n",
    "\n",
    "    def sample_ddpm(self, model, batch_size):\n",
    "        model.eval()\n",
    "        # Algo 2 - Sampling\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((batch_size, 1, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(batch_size) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i>1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1/torch.sqrt(alpha)*(x-((1-alpha)/torch.sqrt(1-alpha_hat))*predicted_noise)+torch.sqrt(beta)*noise\n",
    "            model.train()\n",
    "            x = (x.clamp(-1,1)+1)/2\n",
    "            x = (x*255).type(torch.uint8)\n",
    "            return x\n",
    "\n",
    "    # Normal gaussian noise degradation\n",
    "    def gaussian_noise(self, x0, t):\n",
    "        \"\"\"if len(t.shape) == 1:\n",
    "            t = t[:, None, None, None]\n",
    "\n",
    "        sqrt_alpha_bar = self.alpha_bar[t.squeeze().long().clamp(max=self.steps)].sqrt().to(x0.device)[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = (1 - self.alpha_bar[t.squeeze().long().clamp(max=self.steps)]).sqrt().to(x0.device)[:, None, None, None]\n",
    "\n",
    "        noise = torch.randn_like(x0)\n",
    "        \n",
    "        return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        e = torch.randn_like(x0)\n",
    "        x_t = x0*torch.sqrt(self.alpha_hat[t])[:, None, None, None] + torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]*e \n",
    "        return x_t\n",
    "\n",
    "   \n",
    "\n",
    "    # DEGRADATION FUNCTIONS\n",
    "    def blur(self, x0, t, base_sigma=0.33):\n",
    "        sigma_t = torch.sqrt(t*base_sigma**2).detach().cpu().numpy()\n",
    "        x0_cpu = x0.detach().cpu().numpy()\n",
    "        blurred_imgs = [gaussian_filter(x0_cpu[i], sigma=(0, sigma_t[i], sigma_t[i])) for i in range(len(sigma_t))]\n",
    "        return torch.from_numpy(np.stack(blurred_imgs)).to(device)\n",
    "        # return GaussianBlur(5, sigma_t)\n",
    "\n",
    "    def pixellate(self,x0,t,final_resolution=4): # Super resolution in the paper\n",
    "\n",
    "        original_w,original_h = x0.shape[2],x0.shape[3]\n",
    "                \n",
    "        w,h = original_w,original_h\n",
    "        pixellated_imgs = x0.detach().cpu().numpy()\n",
    "\n",
    "        # Convert to list of numpy arrays\n",
    "        pixellated_imgs = [pixellated_imgs[i].astype(np.float32) for i in range(len(pixellated_imgs))]\n",
    "        while w > final_resolution and h > final_resolution:\n",
    "            if(w//2 < final_resolution and h//2 < final_resolution):\n",
    "                target_width = final_resolution\n",
    "                target_height = final_resolution\n",
    "            else:\n",
    "                target_width = w//2\n",
    "                target_height = h//2\n",
    "            # Downsample the image using average pooling\n",
    "            for i in range(len(pixellated_imgs)):\n",
    "                if t[i] > 0:\n",
    "                    \n",
    "                    pixellated_imgs[i] = downsample_to_fixed_size(pixellated_imgs[i], (target_height, target_width))\n",
    "                    #print(f\"pixellated_imgs[{i}].shape:{pixellated_imgs[i].shape}, target_width:{target_width}, target_height:{target_height}\")\n",
    "             \n",
    "            w,h = w//2,h//2\n",
    "            \n",
    "            t = t - 1    \n",
    "        # Up sample the image to the original resolution (original_w,original_h) using nearest neighbour interpolation\n",
    "\n",
    "        pixellated_imgs = [upsample_nearest(pixellated_imgs[i], original_w, original_h) for i in range(len(pixellated_imgs))]\n",
    "\n",
    "\n",
    "        return torch.from_numpy(np.stack(pixellated_imgs)).to(device)\n",
    "    \n",
    "    def inpainting(self,x0,t,base_variance=1): #Base variance is beta in the paper\n",
    "        w,h = x0.shape[2],x0.shape[3]\n",
    "        center_x,center_y = np.random.randint(0,w, size = x0.shape[0]),np.random.randint(0,h,x0.shape[0])\n",
    "        variance = base_variance + 0.5*t.detach().cpu().numpy()\n",
    "        # 2d gaussian curve with center at rand_x,rand_y and peak value = 1, discretized \n",
    "\n",
    "        x0 = x0.detach().cpu().numpy()\n",
    "        gaussian_mask = np.zeros_like(x0, dtype=float)\n",
    "\n",
    "        y, x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "        gaussian = np.stack([np.exp(-((x - center_x[i])**2 + (y - center_y[i])**2) / (2 * variance[i])) for i in range(len(x0))])\n",
    "        gaussian = 1 - gaussian\n",
    "\n",
    "        # Normalize the gaussian mask\n",
    "        gaussian = gaussian / np.max(gaussian, axis=(1,2), keepdims=True)\n",
    "\n",
    "        # Add extra channel dimension as 2nd dimension\n",
    "        gaussian = gaussian[:,None,:,:]\n",
    "    \n",
    "        # Apply the mask to the image\n",
    "        inpainted_imgs = x0 * gaussian\n",
    "     \n",
    "        return torch.from_numpy(inpainted_imgs).float().to(device)\n",
    "\n",
    "    def snow(self,x0, severity=1):\n",
    "        c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n",
    "            (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n",
    "            (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n",
    "            (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n",
    "            (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n",
    "\n",
    "        x0 = x0.detach().cpu().numpy() / 255.\n",
    "    \n",
    "        snow_layer = np.random.normal(size=x0.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n",
    "\n",
    "        #snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n",
    "        snow_layer[snow_layer < c[3]] = 0\n",
    "\n",
    "        snow_layer = (np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "        snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n",
    "                                cv2.IMREAD_UNCHANGED) / 255.\n",
    "        snow_layer = snow_layer[..., np.newaxis]\n",
    "\n",
    "        x0 = c[6] * x0 + (1 - c[6]) * np.maximum(x0, cv2.cvtColor(x0, cv2.COLOR_RGB2GRAY).reshape(224, 224, 1) * 1.5 + 0.5)\n",
    "        snowified_images =  np.clip(x0 + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n",
    "        \n",
    "        return torch.from_numpy(snowified_images).to(device)\n",
    "\n",
    "    ## Scheduling \n",
    "    def get_beta_schedule(self, timesteps, start=1e-4, end=0.02, schedule_type='linear'):\n",
    "        if schedule_type == 'linear':\n",
    "            return torch.linspace(start, end, timesteps)\n",
    "        elif schedule_type == 'cosine':\n",
    "            steps = torch.arange(timesteps + 1, dtype=torch.float32)\n",
    "            f = lambda t: torch.cos(((t / timesteps) + 0.008) / 1.008 * torch.pi / 2) ** 2\n",
    "            alphas_bar = f(steps) / f(torch.tensor(0.0))\n",
    "            betas = 1 - (alphas_bar[1:] / alphas_bar[:-1])\n",
    "            return torch.clip(betas, 1e-5, 0.999)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown schedule type\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a055",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbda95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, time_dim=256, device=device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 14)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 7)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 4)\n",
    "        \n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 7)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 14)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64,28)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "        \n",
    "    # Sinosoidal encoding - further read\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (10000**(torch.arange(0, channels, 2, device=self.device).float() / channels))\n",
    "        \n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2)*inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels//2)*inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "        \n",
    "        # Decoder\n",
    "        #print(x4.size())\n",
    "        #print(x3.size())\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        return self.outc(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81371f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(1, mid_channels)\n",
    "        self.act = nn.GELU() ## Try Relu, leakyReLU\n",
    "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(1, out_channels)\n",
    "        self.residual = residual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.conv1(x)\n",
    "        x2 = self.norm1(x2)\n",
    "        x2 = self.act(x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = self.norm2(x2)\n",
    "        if self.residual:\n",
    "            return self.act(x+x2)\n",
    "        else:\n",
    "            return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxPool = nn.MaxPool2d(2)\n",
    "        self.doubleConv1 = DoubleConv(in_channels, in_channels, residual=True)\n",
    "        self.doubleConv2 = DoubleConv(in_channels, out_channels)\n",
    "        \n",
    "        self.act = nn.SiLU()\n",
    "        self.linear = nn.Linear(emb_dim, out_channels)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x = self.maxPool(x)\n",
    "        x = self.doubleConv1(x)\n",
    "        x = self.doubleConv2(x)\n",
    "        #print(x.size())\n",
    "        emb = self.act(t)\n",
    "        emb = self.linear(emb)[:, :, None, None].repeat(1,1,x.shape[-2], x.shape[-1])\n",
    "        #print(emb.size())\n",
    "        \n",
    "        return x+emb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18263242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.doubleConv1 = DoubleConv(in_channels, in_channels, residual=True)\n",
    "        self.doubleConv2 = DoubleConv(in_channels, out_channels, in_channels//2)\n",
    "        self.act = nn.SiLU()\n",
    "        self.linear = nn.Linear(emb_dim, out_channels)\n",
    "        \n",
    "    def forward(self, x, skip_x, t):\n",
    "        #print(x.size())\n",
    "        x = self.up(x)\n",
    "        #print(x.size())\n",
    "        if x.shape[-2:] != skip_x.shape[-2:]:\n",
    "            x = nn.functional.interpolate(x, size=skip_x.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            #print(x.size())\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.doubleConv1(x)\n",
    "        x = self.doubleConv2(x)\n",
    "        emb = self.act(t)\n",
    "        emb = self.linear(emb)[:, :, None, None].repeat(1,1,x.shape[-2], x.shape[-1])\n",
    "        return x+emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8564fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.linear = nn.Linear(channels, channels)\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, h*w).permute(0,2,1)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        x = self.ln(attention_value)\n",
    "        x = self.linear(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear(x)\n",
    "        attention_value = x + attention_value\n",
    "        \n",
    "        return attention_value.permute(0, 2, 1).view(b, c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3a2ad",
   "metadata": {},
   "source": [
    "## Function to plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(torch.cat([torch.cat([i for i in images.detach().cpu()], dim=-1)], dim=-2).permute(1,2,0).cpu(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Resize(80),\n",
    "    #torchvision.transforms.RandomResizedCrop(args.img_size, scale=(0.8, 1.0)),\n",
    "    torchvision.transforms.Resize((28, 28)),    # Resize to 28x28 for MNIST\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccff556",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(32)\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    n_val = int(val_pct*n)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdca32c",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "batch_size = 32\n",
    "image_size = 28\n",
    "learning_rate = 1e-3\n",
    "loss_type = 'L1' # 'L1', 'L2', 'SmoothL1', 'Huber'\n",
    "sampling_type = 'gmm' # 'random', 'black', 'real_degraded', 'gmm'\n",
    "\n",
    "# DEGRADATION TYPE\n",
    "degradation_type = 'blur' # 'blur', 'pixellate', 'inpainting', 'snow', 'gaussian'\n",
    "\n",
    "# Steps for diffusion process - changes based on the function used\n",
    "# For pixellate, steps = 4 (MNIST) or 6 (CIFAR10)\n",
    "steps = 200\n",
    "\n",
    "FASHION_MNIST = False\n",
    "\n",
    "# Import dataset\n",
    "if (FASHION_MNIST):\n",
    "    dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, transform=transforms, download=True)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, transform=transforms, download=True)\n",
    "else:\n",
    "    dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transforms, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transforms, download=True)\n",
    "\n",
    "# LIMIT THE DATASET\n",
    "LIMIT = True\n",
    "if LIMIT:\n",
    "    num_samples = 6000\n",
    "    num_test_samples = num_samples // 6\n",
    "\n",
    "    # Randomly select indices for train and test\n",
    "    all_indices = np.random.permutation(len(dataset))\n",
    "    train_indices = all_indices[:num_samples]\n",
    "    test_indices = np.random.permutation(len(test_dataset))[:num_test_samples]\n",
    "\n",
    "    dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "\n",
    "    print(\"LIMITED DATASET\")\n",
    "    print(len(dataset))\n",
    "    print(len(test_dataset))\n",
    "\n",
    "else:\n",
    "    print(\"FULL DATASET\")\n",
    "    print(len(dataset))\n",
    "    print(len(test_dataset))\n",
    "# dataset = torch.utils.data.Subset(dataset, range(1000))\n",
    "# test_dataset = torch.utils.data.Subset(test_dataset, range(1000))\n",
    "\n",
    "\n",
    "train_indices, val_indices = split_indices(len(dataset), 0.2)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset, batch_size, sampler=val_sampler)\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "\n",
    "if loss_type == 'L1':\n",
    "    loss_fn = nn.L1Loss()\n",
    "elif loss_type == 'L2':\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif loss_type == 'SmoothL1':\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "elif loss_type == 'Huber':\n",
    "    loss_fn = nn.HuberLoss()\n",
    "elif loss_type == 'BCE':\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    raise ValueError(\"Invalid loss type. Choose from 'L1', 'L2', 'SmoothL1', 'Huber' or 'BCE'.\")\n",
    "    \n",
    "diffusion = coldDiff(size=image_size, steps = steps, degradation_type=degradation_type)\n",
    "noise_function = diffusion.degradation\n",
    "length = len(train_loader)\n",
    "print(length)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a09299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "def plot_histograms(images, figsize=(32,32)):\n",
    "    \"\"\"\n",
    "    Plot histograms of pixel values for a batch of images in a single row\n",
    "    Args:\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        figsize: Size of the figure to plot\n",
    "    \"\"\"\n",
    "    # Move images to CPU and convert to numpy array\n",
    "    if torch.is_tensor(images):\n",
    "        images = images.cpu().detach().numpy()\n",
    "    \n",
    "    batch_size = images.shape[0]\n",
    "    \n",
    "    # Create figure with subplots in a single row\n",
    "    # Make each subplot square by setting height equal to width\n",
    "    subplot_width = figsize[0] / batch_size\n",
    "    fig, axes = plt.subplots(1, batch_size, figsize=(figsize[0], subplot_width))\n",
    "    \n",
    "    # Plot histogram for each image\n",
    "    for i in range(batch_size):\n",
    "        img = images[i].reshape(-1)  # Flatten single image to 1D\n",
    "        axes[i].hist(img, bins=50, density=True)\n",
    "        axes[i].set_ylim(0, 1)  # Limit height\n",
    "        axes[i].axis('off')  # Remove axes\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f90cf4",
   "metadata": {},
   "source": [
    "### Visualize the dataset and the degradation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in train_loader:\n",
    "    x = x.to(device)\n",
    "    t = diffusion.sample_timesteps(x.shape[0])\n",
    "    print(t,t.shape)\n",
    "    # Plot the images\n",
    "    plot_images(x)\n",
    "    x_t = diffusion.inpainting(x, t)\n",
    "    plot_images(x_t)\n",
    "    x_t = diffusion.pixellate(x, t)\n",
    "    plot_images(x_t)\n",
    "    x_t = diffusion.blur(x, t)\n",
    "    plot_images(x_t)\n",
    "    #x_t = diffusion.snow(x)\n",
    "    #plot_images(x_t)\n",
    "    x_t = diffusion.gaussian_noise(x, t)\n",
    "    plot_images(x_t)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae29684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one image degraded to n time steps\n",
    "\n",
    "factor = steps// batch_size\n",
    "\n",
    "time_steps = range(0, steps, factor)\n",
    "time_steps = time_steps[:batch_size]\n",
    "t = torch.tensor(time_steps).to(device)\n",
    "print(t,t.shape)\n",
    "for x, _ in train_loader:\n",
    "    # Repeat the same image\n",
    "    x = x[0].unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "    x_t = diffusion.degradation(x, t,base_sigma=0.33)\n",
    "    plot_images(x_t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228092a8",
   "metadata": {},
   "source": [
    "# Visualize the degradation level per time step linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(original, degraded):\n",
    "    # Convert tensors to numpy arrays if needed\n",
    "    if torch.is_tensor(original):\n",
    "        original = original.detach().cpu().numpy()\n",
    "    if torch.is_tensor(degraded):\n",
    "        degraded = degraded.detach().cpu().numpy()\n",
    "        \n",
    "    # Ensure same shape\n",
    "    #print(original.shape, degraded.shape)\n",
    "    assert original.shape == degraded.shape, \"Images must have same dimensions\"\n",
    "    \n",
    "    mse = np.mean((original - degraded) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Assume images are normalized to [0,1] range\n",
    "    PIXEL_MAX = 1.0\n",
    "    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = steps// batch_size\n",
    "time_steps = range(0, steps, factor)\n",
    "time_steps = time_steps[:batch_size]\n",
    "\n",
    "t = torch.tensor(time_steps).to(device)\n",
    "print(t)\n",
    "for x, _ in train_loader:\n",
    "    x = x[0]\n",
    "    x = x.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    x_t = diffusion.blur(x, t,base_sigma=0.33)\n",
    "    variance_blur = torch.var(x_t, dim=(1,2,3))\n",
    "    psnr_blur = []\n",
    "    for i in range(batch_size):\n",
    "        psnr_blur.append(psnr(x[0], x_t[i]))\n",
    "    plot_images(x_t)\n",
    "    plot_histograms(x_t)\n",
    "\n",
    "    x_t = diffusion.gaussian_noise(x, t)\n",
    "    variance_noise = torch.var(x_t, dim=(1,2,3))\n",
    "    psnr_noise = []\n",
    "    for i in range(batch_size):\n",
    "        psnr_noise.append(psnr(x[0], x_t[i]))\n",
    "    plot_images(x_t)\n",
    "    plot_histograms(x_t)\n",
    "\n",
    "    plt.plot(time_steps, variance_blur.to(\"cpu\"), label='Blur')\n",
    "    plt.plot(time_steps, variance_noise.to(\"cpu\"), label='Gaussian Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(time_steps, psnr_blur, label='PSNR Blur')\n",
    "    plt.plot(time_steps, psnr_noise, label='PSNR Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc8f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "factor = steps// batch_size\n",
    "time_steps = range(0, steps, factor)\n",
    "time_steps = time_steps[:batch_size]\n",
    "blur_sigmas = [x/10 for x in range(1,9,1)]\n",
    "t = torch.tensor(time_steps).to(device)\n",
    "for x, _ in train_loader:\n",
    "    x = x[0]\n",
    "    x = x.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    all_psnr_blur = []\n",
    "    all_ssim_blur = []\n",
    "    for j in blur_sigmas:\n",
    "        psnr_blur = []\n",
    "        ssim_blur = []\n",
    "        x_t = diffusion.blur(x, t,base_sigma=j)\n",
    "        for i in range(batch_size):\n",
    "            if i >= len(x_t):\n",
    "                break\n",
    "            psnr_blur.append(psnr(x[0], x_t[i]))\n",
    "            x_0_numpy = x[0].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            x_t_numpy = x_t[i].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            data_range = x_0_numpy.max() - x_0_numpy.min()\n",
    "\n",
    "            ssim_blur.append(ssim(x_0_numpy, x_t_numpy,data_range=data_range))\n",
    "\n",
    "        all_psnr_blur.append(psnr_blur)\n",
    "        all_ssim_blur.append(ssim_blur)\n",
    "\n",
    "    x_t = diffusion.gaussian_noise(x, t)\n",
    "    psnr_noise = []\n",
    "    ssim_noise = []\n",
    "    for i in range(batch_size):\n",
    "        if i >= len(x_t):\n",
    "            break\n",
    "        psnr_noise.append(psnr(x[0], x_t[i]))\n",
    "        x_0_numpy = x[0].unsqueeze(0).cpu().numpy()[0][0]\n",
    "        x_t_numpy = x_t[i].unsqueeze(0).cpu().numpy()[0][0]\n",
    "        data_range = x_0_numpy.max() - x_0_numpy.min()\n",
    "        ssim_noise.append(ssim(x_0_numpy, x_t_numpy,data_range=data_range))\n",
    "    \n",
    "    # Plot PSNR values\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for idx, sigma in enumerate(blur_sigmas):\n",
    "        plt.plot(time_steps, all_psnr_blur[idx], label=f'PSNR Blur σ={sigma:.2f}')\n",
    "    plt.plot(time_steps, psnr_noise, label='PSNR Gaussian Noise', linestyle='--')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('PSNR')\n",
    "    plt.title('PSNR Comparison with input image: Blur vs Gaussian Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot SSIM values  \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for idx, sigma in enumerate(blur_sigmas):\n",
    "        plt.plot(time_steps, all_ssim_blur[idx], label=f'SSIM Blur σ={sigma:.2f}')\n",
    "    plt.plot(time_steps, ssim_noise, label='SSIM Gaussian Noise', linestyle='--')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.title('SSIM Comparison with input image: Blur vs Gaussian Noise') \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aded5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = steps// batch_size\n",
    "time_steps = range(0, steps, factor)\n",
    "time_steps = time_steps[:batch_size]\n",
    "blur_sigmas = [x/20 for x in range(2,8,1)]\n",
    "t = torch.tensor(time_steps).to(device)\n",
    "for x, _ in train_loader:\n",
    "    x = x[0]\n",
    "    x = x.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    all_psnr_blur = []\n",
    "    all_ssim_blur = []\n",
    "    for j in blur_sigmas:\n",
    "        psnr_blur = []\n",
    "        ssim_blur = []\n",
    "        x_t = diffusion.blur(x, t, base_sigma=j)\n",
    "        for i in range(1, batch_size):\n",
    "            psnr_blur.append(psnr(x_t[i-1], x_t[i]))\n",
    "            x_prev_numpy = x_t[i-1].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            x_curr_numpy = x_t[i].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            data_range = x_prev_numpy.max() - x_prev_numpy.min()\n",
    "            ssim_blur.append(ssim(x_prev_numpy, x_curr_numpy, data_range=data_range))\n",
    "\n",
    "        all_psnr_blur.append(psnr_blur)\n",
    "        all_ssim_blur.append(ssim_blur)\n",
    "\n",
    "    x_t = diffusion.gaussian_noise(x, t)\n",
    "    psnr_noise = []\n",
    "    ssim_noise = []\n",
    "    for i in range(1, batch_size):\n",
    "        psnr_noise.append(psnr(x_t[i-1], x_t[i]))\n",
    "        x_prev_numpy = x_t[i-1].unsqueeze(0).cpu().numpy()[0][0]\n",
    "        x_curr_numpy = x_t[i].unsqueeze(0).cpu().numpy()[0][0]\n",
    "        data_range = x_prev_numpy.max() - x_prev_numpy.min()\n",
    "        ssim_noise.append(ssim(x_prev_numpy, x_curr_numpy, data_range=data_range))\n",
    "    \n",
    "    # Plot PSNR values for different blur sigmas and noise\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for idx, sigma in enumerate(blur_sigmas):\n",
    "        plt.plot(time_steps[1:], all_psnr_blur[idx], label=f'PSNR Blur σ={sigma:.2f}')\n",
    "    plt.plot(time_steps[1:], psnr_noise, label='PSNR Gaussian Noise', linestyle='--')\n",
    "    plt.xlabel('Time Steps') \n",
    "    plt.ylabel('PSNR')\n",
    "    plt.title('PSNR Comparison between consecutive images: Blur vs Gaussian Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot SSIM values for different blur sigmas and noise\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for idx, sigma in enumerate(blur_sigmas):\n",
    "        plt.plot(time_steps[1:], all_ssim_blur[idx], label=f'SSIM Blur σ={sigma:.2f}')\n",
    "    plt.plot(time_steps[1:], ssim_noise, label='SSIM Gaussian Noise', linestyle='--')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.title('SSIM Comparison between consecutive images: Blur vs Gaussian Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [10,20,50,100,250,500,1000]\n",
    "blur_sigmas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "result = []\n",
    "t = torch.tensor(time_steps).to(device)\n",
    "\n",
    "for x, _ in train_loader:\n",
    "    x = x[0]\n",
    "    x = x.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    for j in blur_sigmas:\n",
    "        psnr_blur = []\n",
    "        ssim_blur = []\n",
    "        x_t = diffusion.blur(x, t, base_sigma=j)\n",
    "        for i in range(batch_size):\n",
    "            if i >= len(x_t):\n",
    "                print(f\"Index {i} is out of bounds for x_t with length {len(x_t)}\")\n",
    "                continue\n",
    "            psnr_blur.append(psnr(x[0], x_t[i]))\n",
    "            x_0_numpy = x[0].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            x_t_numpy = x_t[i].unsqueeze(0).cpu().numpy()[0][0]\n",
    "            data_range = x_0_numpy.max() - x_0_numpy.min()\n",
    "            ssim_blur.append(ssim(x_0_numpy, x_t_numpy, data_range=data_range))\n",
    "        \n",
    "        result.append((j, psnr_blur, ssim_blur))\n",
    "\n",
    "    break\n",
    "\n",
    "# Prepare data for heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "psnr_heatmap = np.zeros((len(time_steps), len(blur_sigmas)))\n",
    "ssim_heatmap = np.zeros((len(time_steps), len(blur_sigmas)))\n",
    "\n",
    "for idx, (j, psnr_blur, ssim_blur) in enumerate(result):\n",
    "    psnr_heatmap[:, idx] = psnr_blur\n",
    "    ssim_heatmap[:, idx] = ssim_blur\n",
    "\n",
    "# Plot heatmap for PSNR\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(psnr_heatmap, annot=True, fmt=\".2f\", xticklabels=blur_sigmas, yticklabels=time_steps, cmap='viridis', cbar_kws={'label': 'PSNR'})\n",
    "plt.title('PSNR Heatmap')\n",
    "plt.xlabel('Blur Sigma')\n",
    "plt.ylabel('Time Steps')\n",
    "plt.show()\n",
    "\n",
    "# Plot heatmap for SSIM\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(ssim_heatmap, annot=True, fmt=\".2f\", xticklabels=blur_sigmas, yticklabels=time_steps, cmap='viridis', cbar_kws={'label': 'SSIM'})\n",
    "plt.title('SSIM Heatmap')\n",
    "plt.xlabel('Blur Sigma')\n",
    "plt.ylabel('Time Steps')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize results array\n",
    "psnr_results = np.zeros((len(t), len(blur_sigmas)))  # Switch dimensions\n",
    "\n",
    "for x, _ in train_loader:\n",
    "    x = x[0]\n",
    "    x = x.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    for step_idx, step in enumerate(t):  # Iterate over steps first\n",
    "        previous_x_t = None  # Initialize the previous step tensor\n",
    "        for sigma_idx, j in enumerate(blur_sigmas):  # Iterate over sigmas\n",
    "            # Apply blur degradation\n",
    "            current_x_t = diffusion.blur(x, t=torch.tensor([step]).to(device), base_sigma=j)\n",
    "            \n",
    "            # Calculate PSNR between the current and previous sigma\n",
    "            if previous_x_t is not None:\n",
    "                psnr_value = psnr(previous_x_t, current_x_t)\n",
    "                psnr_results[step_idx, sigma_idx] = psnr_value\n",
    "            \n",
    "            # Update the previous sigma tensor\n",
    "            previous_x_t = current_x_t\n",
    "\n",
    "    break\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(psnr_results, annot=True, fmt=\".2f\", xticklabels=blur_sigmas, yticklabels=t.cpu().numpy(), cmap=\"viridis\")\n",
    "plt.title(\"Average PSNR Heatmap (Steps vs Blur Sigma)\")\n",
    "plt.xlabel(\"Blur Sigma\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc9a78",
   "metadata": {},
   "source": [
    "# Fit a GMM to the dataset (for the blur degradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_GMM = True\n",
    "\n",
    "if FIT_GMM:\n",
    "    all_blurred = []\n",
    "\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        batch_size_t = x.size(0)\n",
    "        \n",
    "        t = torch.full((batch_size_t,), steps).long().to(device)  # Full degradation (T steps)\n",
    "        x_blurred = diffusion.degradation(x, t)\n",
    "        \n",
    "        all_blurred.append(x_blurred.cpu())\n",
    "\n",
    "    # Stack all blurred images\n",
    "    all_blurred = torch.cat(all_blurred, dim=0)  # shape: [N, 1, 28, 28]\n",
    "    all_blurred = all_blurred.squeeze(1)  # [N, 28, 28]\n",
    "\n",
    "    all_blurred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_GMM:\n",
    "\n",
    "    X = all_blurred.reshape(all_blurred.shape[0], -1)  # [N, 784]\n",
    "\n",
    "    X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "if FIT_GMM:\n",
    "\n",
    "    \n",
    "\n",
    "    # Choose number of components (1–5 is usually enough for MNIST)\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)\n",
    "    gmm.fit(X.numpy())  # convert to numpy\n",
    "\n",
    "    # Save the GMM model\n",
    "    print(\"Saving GMM model\")\n",
    "    joblib.dump(gmm, 'gmm_model_steps100_sigma1_6k.pkl')\n",
    "\n",
    "else:\n",
    "# Load the GMM model\n",
    "    print(\"Loading GMM model\")\n",
    "    gmm = joblib.load('gmm_model_steps100_sigma1_6k.pkl')\n",
    "    print(gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1bf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample new points from the GMM\n",
    "samples, _ = gmm.sample(batch_size)  # shape [batch_size, 784]\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "torch.from_numpy(samples).view(batch_size, 1, 28, 28).shape  # Reshape to [batch_size, 1, 28, 28]\n",
    "\n",
    "# Reshape back into images\n",
    "samples = torch.from_numpy(samples).float().view(batch_size, 1, 28, 28).to(device)\n",
    "\n",
    "# Now use samples as initial x_T\n",
    "x_prev = samples\n",
    "\n",
    "# Plot the generated images\n",
    "plot_images(x_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72049e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5453bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DELETE_PREVIOUS_RUN = False\n",
    "\n",
    "run_name = f'coldDiffusion_{degradation_type}_s{steps}_b{batch_size}_{sampling_type}_{learning_rate:.1e}_e{EPOCHS}_{loss_type}_{len(dataset)}_LIMITED'\n",
    "if scheduler is not None:\n",
    "    run_name += f'_scheduler_{scheduler.__class__.__name__}'\n",
    "\n",
    "if DELETE_PREVIOUS_RUN:\n",
    "    # Delete the previous run folder if it exists\n",
    "    if os.path.exists('runs/' + run_name):\n",
    "        shutil.rmtree('runs/' + run_name)\n",
    "        print(f\"Deleted previous run folder: runs/{run_name}\")\n",
    "\n",
    "else:\n",
    "    # Add a version number to the run name if it already exists\n",
    "    version = 1\n",
    "    while os.path.exists('runs/' + run_name):\n",
    "        run_name = f'coldDiffusion_{degradation_type}_s{steps}_b{batch_size}_{sampling_type}_{learning_rate:.1e}_e{EPOCHS}_{loss_type}_{len(dataset)}_LIMITED'\n",
    "        if scheduler is not None:\n",
    "            run_name += f'_scheduler_{scheduler.__class__.__name__}'\n",
    "        run_name += f'_v{version}'\n",
    "        version += 1\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('runs/' + run_name)\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda clear cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b550101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the noise function to be used for training\n",
    "\n",
    "# noise_function = diffusion.vignette\n",
    "# noise_function = diffusion.pixellate\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch \",epoch)\n",
    "    train_loss = 0\n",
    "    for x, _ in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        t = diffusion.sample_timesteps(x.shape[0]).to(device)\n",
    "        x_t = noise_function(x, t)\n",
    "        pred = model(x_t, t)\n",
    "        loss = loss_fn(pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for x, _ in val_loader:\n",
    "\n",
    "            #print(type(x))\n",
    "            #print(x.shape)\n",
    "            x = x.to(device)\n",
    "            t = diffusion.sample_timesteps(x.shape[0]).to(device)\n",
    "            x_t = noise_function(x, t)\n",
    "            pred = model(x_t, t)\n",
    "            batch_val_loss = loss_fn(pred, x)\n",
    "            val_loss += batch_val_loss.item()\n",
    "\n",
    "            del x, x_t, t, pred, batch_val_loss\n",
    "        scheduler.step(val_loss/len(val_loader))\n",
    "        # Sample only every n epochs for opt purposes\n",
    "        if epoch%3 == 0:\n",
    "            # Sample 1000 images using the GMM\n",
    "            sampled_images = []\n",
    "            for _ in range(1000 // batch_size):\n",
    "                images = diffusion.sample(model, batch_size=batch_size, initial_image='gmm', gmm=gmm, data_loader=val_loader)\n",
    "                sampled_images.append(images)\n",
    "            \n",
    "            # Concatenate all sampled images\n",
    "            sampled_images = torch.cat(sampled_images, dim=0)\n",
    "            \n",
    "            # Save sampled images to a directory for FID calculations\n",
    "            fid_dir = \"./fid_samples\"\n",
    "            if not os.path.exists(fid_dir):\n",
    "                os.makedirs(fid_dir)\n",
    "            \n",
    "            for i, img in enumerate(sampled_images):\n",
    "                save_image(img, os.path.join(fid_dir, f'sampled_{i}.png'))\n",
    "        \n",
    "    # Save loss\n",
    "    avg_train_loss = train_loss/len(train_loader)\n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Learning Rate: {scheduler.get_last_lr()[0]:.10f}\")\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Models/DM/' + run_name + '.pth')\n",
    "print(\"Model saved to ./Models/DM/\" + run_name + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98387b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "import joblib\n",
    "\n",
    "LOAD_MODEL = True\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load('./Models/DM/coldDiffusion_blur_s40_b32_gmm_1.0e-04_e100_L1_6000_scheduler_ReduceLROnPlateau.pth'))\n",
    "    gmm = joblib.load('gmm_model_steps40_sigma1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = coldDiff()\n",
    "sampled_images = diff2.sample(model, batch_size=5, initial_image='gmm', gmm=gmm, data_loader=val_loader)\n",
    "plot_images(sampled_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0bd29",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638173b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid.inception import InceptionV3\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import inception_v3\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1-mu2\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff.dot(diff) + np.trace(sigma1+sigma2- 2*covmean)\n",
    "    return fid\n",
    "\n",
    "def get_activations(images, model, batch_size=32, dims=2048, device='cuda'):\n",
    "    model.eval()\n",
    "    pred_arr = np.empty((len(images), dims))\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size)):\n",
    "            start = i\n",
    "            end = i+batch_size\n",
    "            batch = images[start:end].to(device)\n",
    "            #print(batch.size())\n",
    "            #raise\n",
    "            pred = model(batch)[0]\n",
    "            pred = F.adaptive_avg_pool2d(pred, output_size=(1,1))\n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr[start:end] = pred\n",
    "    return pred_arr\n",
    "\n",
    "#inception_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "#inception_model.fc = torch.nn.Identity()\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "inception_model = InceptionV3([block_idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "LOAD_MODEL = True\n",
    "if LOAD_MODEL:\n",
    "    model = UNet().to(device)\n",
    "    model.load_state_dict(torch.load('./Models/DM/coldDiffusion_blur_s40_b32_gmm_1.0e-04_e100_L1_6000_scheduler_ReduceLROnPlateau.pth'))\n",
    "    print(\"Model loaded from ./Models/DM/coldDiffusion_blur_s40_b32_gmm_1.0e-04_e100_L1_6000_scheduler_ReduceLROnPlateau.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d591f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = []\n",
    "generated_images = []\n",
    "\n",
    "# \n",
    "\n",
    "model.eval()\n",
    "FID_LIMIT = 1000 # Limit the number of images to be generated for FID calculation\n",
    "for i, (x, t) in tqdm(enumerate(train_loader)):\n",
    "    if i > FID_LIMIT//x.shape[0]:\n",
    "        break\n",
    "    original_images.append(x)\n",
    "    \n",
    "    sampled = diffusion.sample(model, batch_size=x.shape[0], initial_image='gmm', gmm=gmm, data_loader=val_loader)\n",
    "    generated_images.append(sampled)\n",
    "\n",
    "original_images = torch.cat(original_images, dim=0)\n",
    "generated_images = torch.cat(generated_images, dim=0)\n",
    "\n",
    "# Save generated images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "dir = \"./cold_diff_samples_ALL_DATA\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "for i, img in enumerate(generated_images):\n",
    "    save_image(img, os.path.join(dir, f'generated_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c454a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = torch.cat(original_images, dim=0).to(device)\n",
    "generated_images = torch.cat(generated_images, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images.size(), generated_images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc05d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images\n",
    "plot_images(original_images[:5])\n",
    "plot_images(generated_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Repeat the single channel\n",
    "\n",
    "original_transformed = []\n",
    "generated_transformed = []\n",
    "\n",
    "# Apply transform\n",
    "for i in range(len(generated_images)):\n",
    "    original_transformed.append(transform(original_images[i]))\n",
    "    generated_transformed.append(transform(generated_images[i]))\n",
    "    \n",
    "# Stack back into batches\n",
    "original_transformed = torch.stack(original_transformed)\n",
    "generated_transformed = torch.stack(generated_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb66a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_transformed.size(), generated_transformed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd39a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_activations = get_activations(original_transformed, inception_model, device=device)\n",
    "generated_activations = get_activations(generated_transformed, inception_model, device=device)\n",
    "\n",
    "# Calculate mean and covariance of the activations\n",
    "mu1 = np.mean(original_activations, axis=0)\n",
    "sigma1 = np.cov(original_activations, rowvar=False)\n",
    "mu2 = np.mean(generated_activations, axis=0)\n",
    "sigma2 = np.cov(generated_activations, rowvar=False)\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid(mu1, sigma1, mu2, sigma2)\n",
    "print('FID score:', fid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c8bd4",
   "metadata": {},
   "source": [
    "# FID using torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fcc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images to 0-255 from -1 to 1\n",
    "original_images = (original_images + 1) / 2 * 255\n",
    "generated_images = (generated_images + 1) / 2 * 255\n",
    "\n",
    "# Convert to uint8\n",
    "original_images = original_images.type(torch.uint8)\n",
    "generated_images = generated_images.type(torch.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break generated images into smaller batches for FID\n",
    "batch_size = 32  # or even 16 if needed\n",
    "for i in range(0, len(generated_images), batch_size):\n",
    "    batch = generated_images[i:i+batch_size]\n",
    "\n",
    "    fid.update(batch, real=False)\n",
    "\n",
    "for i in range(0, len(original_images), batch_size):\n",
    "    batch = original_images[i:i+batch_size]\n",
    "    fid.update(batch, real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a902134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize FID\n",
    "    fid = FrechetInceptionDistance(feature=2048)\n",
    "\n",
    "    # Update real images\n",
    "    fid.update(original_images, real=True)\n",
    "\n",
    "    # Update generated images\n",
    "    fid.update(generated_images, real=False)\n",
    "\n",
    "    # Compute FID\n",
    "    fid_score = fid.compute()\n",
    "    print(\"FID:\", fid_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc84d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(torchmetrics.image.fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db78d2",
   "metadata": {},
   "source": [
    "### Use GMM to sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589200b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transforms, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "diffusion = coldDiff()\n",
    "\n",
    "fully_degraded_images = []\n",
    "\n",
    "for x, _ in test_loader:\n",
    "    x = x.to(device)\n",
    "    t = diffusion.sample_timesteps(x.shape[0]).to(device)\n",
    "    x_t = diffusion.blur(x, t)\n",
    "    flattened_images = x_t.view(x_t.size(0), -1).detach().cpu().numpy()\n",
    "    fully_degraded_images.extend(flattened_images)\n",
    "\n",
    "    \n",
    "fully_degraded_images = np.array(fully_degraded_images)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(fully_degraded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "sampled_images = gmm.sample(n_samples)[0]\n",
    "initial_images = torch.tensor(sampled_images).float().to(device)\n",
    "initial_images = initial_images.view(n_samples, 1, 28, 28)\n",
    "plot_images(initial_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e78fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e486c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_intensities = gmm.sample(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gmm, 'Models/DM/gmm_blur_coldDiff.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba0697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8216d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b759dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152f08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d185f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d38bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
